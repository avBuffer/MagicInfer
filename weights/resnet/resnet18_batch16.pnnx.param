7767517
51 50
pnnx.Input               pnnx_input_0             0 1 0 #0=(16,3,224,224)f32
nn.Conv2d                convbn2d_0               1 1 0 1 bias=True dilation=(1,1) groups=1 in_channels=3 kernel_size=(7,7) out_channels=64 padding=(3,3) padding_mode=zeros stride=(2,2) @bias=(64)f32 @weight=(64,3,7,7)f32 $input=0 #0=(16,3,224,224)f32 #1=(16,64,112,112)f32
nn.ReLU                  relu                     1 1 1 2 #1=(16,64,112,112)f32 #2=(16,64,112,112)f32
nn.MaxPool2d             maxpool                  1 1 2 3 ceil_mode=False dilation=(1,1) kernel_size=(3,3) padding=(1,1) return_indices=False stride=(2,2) #2=(16,64,112,112)f32 #3=(16,64,56,56)f32
nn.Conv2d                convbn2d_1               1 1 3 4 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 $input=3 #3=(16,64,56,56)f32 #4=(16,64,56,56)f32
nn.ReLU                  layer1.0.relu            1 1 4 5 #4=(16,64,56,56)f32 #5=(16,64,56,56)f32
nn.Conv2d                convbn2d_2               1 1 5 6 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 $input=5 #5=(16,64,56,56)f32 #6=(16,64,56,56)f32
pnnx.Expression          pnnx_expr_14             2 1 6 3 7 expr=add(@0,@1) #6=(16,64,56,56)f32 #3=(16,64,56,56)f32 #7=(16,64,56,56)f32
nn.ReLU                  pnnx_unique_0            1 1 7 8 #7=(16,64,56,56)f32 #8=(16,64,56,56)f32
nn.Conv2d                convbn2d_3               1 1 8 9 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 $input=8 #8=(16,64,56,56)f32 #9=(16,64,56,56)f32
nn.ReLU                  layer1.1.relu            1 1 9 10 #9=(16,64,56,56)f32 #10=(16,64,56,56)f32
nn.Conv2d                convbn2d_4               1 1 10 11 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 $input=10 #10=(16,64,56,56)f32 #11=(16,64,56,56)f32
pnnx.Expression          pnnx_expr_12             2 1 11 8 12 expr=add(@0,@1) #11=(16,64,56,56)f32 #8=(16,64,56,56)f32 #12=(16,64,56,56)f32
nn.ReLU                  pnnx_unique_1            1 1 12 13 #12=(16,64,56,56)f32 #13=(16,64,56,56)f32
nn.Conv2d                convbn2d_5               1 1 13 14 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(128)f32 @weight=(128,64,3,3)f32 $input=13 #13=(16,64,56,56)f32 #14=(16,128,28,28)f32
nn.ReLU                  layer2.0.relu            1 1 14 15 #14=(16,128,28,28)f32 #15=(16,128,28,28)f32
nn.Conv2d                convbn2d_7               1 1 13 16 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=128 padding=(0,0) padding_mode=zeros stride=(2,2) @bias=(128)f32 @weight=(128,64,1,1)f32 $input=13 #13=(16,64,56,56)f32 #16=(16,128,28,28)f32
nn.Conv2d                convbn2d_6               1 1 15 17 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 $input=15 #15=(16,128,28,28)f32 #17=(16,128,28,28)f32
pnnx.Expression          pnnx_expr_10             2 1 17 16 18 expr=add(@0,@1) #17=(16,128,28,28)f32 #16=(16,128,28,28)f32 #18=(16,128,28,28)f32
nn.ReLU                  pnnx_unique_2            1 1 18 19 #18=(16,128,28,28)f32 #19=(16,128,28,28)f32
nn.Conv2d                convbn2d_8               1 1 19 20 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 $input=19 #19=(16,128,28,28)f32 #20=(16,128,28,28)f32
nn.ReLU                  layer2.1.relu            1 1 20 21 #20=(16,128,28,28)f32 #21=(16,128,28,28)f32
nn.Conv2d                convbn2d_9               1 1 21 22 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 $input=21 #21=(16,128,28,28)f32 #22=(16,128,28,28)f32
pnnx.Expression          pnnx_expr_8              2 1 22 19 23 expr=add(@0,@1) #22=(16,128,28,28)f32 #19=(16,128,28,28)f32 #23=(16,128,28,28)f32
nn.ReLU                  pnnx_unique_3            1 1 23 24 #23=(16,128,28,28)f32 #24=(16,128,28,28)f32
nn.Conv2d                convbn2d_10              1 1 24 25 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(256)f32 @weight=(256,128,3,3)f32 $input=24 #24=(16,128,28,28)f32 #25=(16,256,14,14)f32
nn.ReLU                  layer3.0.relu            1 1 25 26 #25=(16,256,14,14)f32 #26=(16,256,14,14)f32
nn.Conv2d                convbn2d_12              1 1 24 27 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=256 padding=(0,0) padding_mode=zeros stride=(2,2) @bias=(256)f32 @weight=(256,128,1,1)f32 $input=24 #24=(16,128,28,28)f32 #27=(16,256,14,14)f32
nn.Conv2d                convbn2d_11              1 1 26 28 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 $input=26 #26=(16,256,14,14)f32 #28=(16,256,14,14)f32
pnnx.Expression          pnnx_expr_6              2 1 28 27 29 expr=add(@0,@1) #28=(16,256,14,14)f32 #27=(16,256,14,14)f32 #29=(16,256,14,14)f32
nn.ReLU                  pnnx_unique_4            1 1 29 30 #29=(16,256,14,14)f32 #30=(16,256,14,14)f32
nn.Conv2d                convbn2d_13              1 1 30 31 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 $input=30 #30=(16,256,14,14)f32 #31=(16,256,14,14)f32
nn.ReLU                  layer3.1.relu            1 1 31 32 #31=(16,256,14,14)f32 #32=(16,256,14,14)f32
nn.Conv2d                convbn2d_14              1 1 32 33 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 $input=32 #32=(16,256,14,14)f32 #33=(16,256,14,14)f32
pnnx.Expression          pnnx_expr_4              2 1 33 30 34 expr=add(@0,@1) #33=(16,256,14,14)f32 #30=(16,256,14,14)f32 #34=(16,256,14,14)f32
nn.ReLU                  pnnx_unique_5            1 1 34 35 #34=(16,256,14,14)f32 #35=(16,256,14,14)f32
nn.Conv2d                convbn2d_15              1 1 35 36 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(2,2) @bias=(512)f32 @weight=(512,256,3,3)f32 $input=35 #35=(16,256,14,14)f32 #36=(16,512,7,7)f32
nn.ReLU                  layer4.0.relu            1 1 36 37 #36=(16,512,7,7)f32 #37=(16,512,7,7)f32
nn.Conv2d                convbn2d_17              1 1 35 38 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=512 padding=(0,0) padding_mode=zeros stride=(2,2) @bias=(512)f32 @weight=(512,256,1,1)f32 $input=35 #35=(16,256,14,14)f32 #38=(16,512,7,7)f32
nn.Conv2d                convbn2d_16              1 1 37 39 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(512)f32 @weight=(512,512,3,3)f32 $input=37 #37=(16,512,7,7)f32 #39=(16,512,7,7)f32
pnnx.Expression          pnnx_expr_2              2 1 39 38 40 expr=add(@0,@1) #39=(16,512,7,7)f32 #38=(16,512,7,7)f32 #40=(16,512,7,7)f32
nn.ReLU                  pnnx_unique_6            1 1 40 41 #40=(16,512,7,7)f32 #41=(16,512,7,7)f32
nn.Conv2d                convbn2d_18              1 1 41 42 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(512)f32 @weight=(512,512,3,3)f32 $input=41 #41=(16,512,7,7)f32 #42=(16,512,7,7)f32
nn.ReLU                  layer4.1.relu            1 1 42 43 #42=(16,512,7,7)f32 #43=(16,512,7,7)f32
nn.Conv2d                convbn2d_19              1 1 43 44 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(3,3) out_channels=512 padding=(1,1) padding_mode=zeros stride=(1,1) @bias=(512)f32 @weight=(512,512,3,3)f32 $input=43 #43=(16,512,7,7)f32 #44=(16,512,7,7)f32
pnnx.Expression          pnnx_expr_0              2 1 44 41 45 expr=add(@0,@1) #44=(16,512,7,7)f32 #41=(16,512,7,7)f32 #45=(16,512,7,7)f32
nn.ReLU                  pnnx_unique_7            1 1 45 46 #45=(16,512,7,7)f32 #46=(16,512,7,7)f32
nn.AdaptiveAvgPool2d     avgpool                  1 1 46 47 output_size=(1,1) #46=(16,512,7,7)f32 #47=(16,512,1,1)f32
torch.flatten            torch.flatten_0          1 1 47 48 end_dim=-1 start_dim=1 $input=47 #47=(16,512,1,1)f32 #48=(16,512)f32
nn.Linear                fc                       1 1 48 49 bias=True in_features=512 out_features=1000 @bias=(1000)f32 @weight=(1000,512)f32 #48=(16,512)f32 #49=(16,1000)f32
pnnx.Output              pnnx_output_0            1 0 49 #49=(16,1000)f32
